{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "86Mvu7JNfYJf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.layers import Input, Dense, LSTM, Conv1D, MaxPooling1D, Flatten\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# load the dataset\n",
        "df = pd.read_csv(\"/content/TTM.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will preprocess the data by selecting the 6 features and 10 previous days as input, and the adjusted closing price as the output:"
      ],
      "metadata": {
        "id": "iiUMdjEWfdqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\"]]\n",
        "\n",
        "# create a new dataframe with the input and output\n",
        "data = []\n",
        "n_steps = 10\n",
        "n_features = 6\n",
        "for i in range(n_steps, len(df)):\n",
        "    x = df.iloc[i-n_steps:i, :n_features].values\n",
        "    y = df.iloc[i, n_features]\n",
        "    data.append([x, y])\n",
        "\n",
        "data = np.array(data)"
      ],
      "metadata": {
        "id": "KrAmjTqlfe9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then split the data into training and testing sets:\n",
        "\n"
      ],
      "metadata": {
        "id": "BDzvAuvWfh4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[:, 0], data[:, 1], test_size=0.2)"
      ],
      "metadata": {
        "id": "6k-0TKu-flUg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will define the CNN-LSTM model:\n",
        "\n"
      ],
      "metadata": {
        "id": "2G1miHCQfl53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the CNN-LSTM model\n",
        "inputs = Input(shape=(n_steps, n_features))\n",
        "cnn = Conv1D(filters=64, kernel_size=2, activation='relu')(inputs)\n",
        "cnn = MaxPooling1D(pool_size=2)(cnn)\n",
        "cnn = Flatten()(cnn)\n",
        "lstm = LSTM(64)(cnn)\n",
        "outputs = Dense(1)(lstm)\n",
        "model = Model(inputs, outputs)\n"
      ],
      "metadata": {
        "id": "pzyIGUnffpGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will compile and fit the model to the training data, and evaluate it on the testing data:\n",
        "\n"
      ],
      "metadata": {
        "id": "5oysg8QqfrEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile and fit the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
        "\n",
        "# evaluate the model on the testing data\n",
        "score = model.evaluate(X_test, y_test)\n",
        "print(\"Test loss:\", score)\n"
      ],
      "metadata": {
        "id": "7AxqW1tOfsk-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}